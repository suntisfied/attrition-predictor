{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Predicting Probability of Quitting and Providing Prevention Hints"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Import Necessary Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 51,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import numpy as np\n",
                "import time\n",
                "import os\n",
                "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, KFold, cross_val_score\n",
                "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder\n",
                "from sklearn.compose import ColumnTransformer\n",
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score, confusion_matrix, ConfusionMatrixDisplay\n",
                "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
                "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, RandomForestRegressor, GradientBoostingRegressor\n",
                "from xgboost import XGBClassifier, XGBRegressor\n",
                "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
                "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
                "from sklearn.tree import DecisionTreeRegressor\n",
                "from imblearn.over_sampling import RandomOverSampler, SMOTE, BorderlineSMOTE, ADASYN\n",
                "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
                "import statsmodels.api as sm\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load the Dataset and check duplicates"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 52,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Number of additional duplicate copies (excluding the first occurrence): 3008\n",
                        "Duplicate Rows (including originals):\n",
                        "       satisfaction_level  last_evaluation  number_project  \\\n",
                        "30                   0.09             0.62               6   \n",
                        "12030                0.09             0.62               6   \n",
                        "14241                0.09             0.62               6   \n",
                        "71                   0.09             0.77               5   \n",
                        "12071                0.09             0.77               5   \n",
                        "...                   ...              ...             ...   \n",
                        "13089                1.00             0.88               6   \n",
                        "11375                1.00             0.93               5   \n",
                        "13586                1.00             0.93               5   \n",
                        "10691                1.00             0.93               5   \n",
                        "12902                1.00             0.93               5   \n",
                        "\n",
                        "       average_montly_hours  time_spend_company  Work_accident  left  \\\n",
                        "30                      294                   4              0     1   \n",
                        "12030                   294                   4              0     1   \n",
                        "14241                   294                   4              0     1   \n",
                        "71                      275                   4              0     1   \n",
                        "12071                   275                   4              0     1   \n",
                        "...                     ...                 ...            ...   ...   \n",
                        "13089                   201                   4              0     0   \n",
                        "11375                   167                   3              0     0   \n",
                        "13586                   167                   3              0     0   \n",
                        "10691                   231                   2              0     0   \n",
                        "12902                   231                   2              0     0   \n",
                        "\n",
                        "       promotion_last_5years  Departments  salary  \n",
                        "30                         0   accounting     low  \n",
                        "12030                      0   accounting     low  \n",
                        "14241                      0   accounting     low  \n",
                        "71                         0  product_mng  medium  \n",
                        "12071                      0  product_mng  medium  \n",
                        "...                      ...          ...     ...  \n",
                        "13089                      0    technical     low  \n",
                        "11375                      0        sales  medium  \n",
                        "13586                      0        sales  medium  \n",
                        "10691                      0    marketing  medium  \n",
                        "12902                      0    marketing  medium  \n",
                        "\n",
                        "[5346 rows x 10 columns]\n",
                        "All duplicate rows (including originals) have been saved to \"duplicate_rows.csv\".\n",
                        "Number of rows after removing duplicates: 11991\n"
                    ]
                }
            ],
            "source": [
                "# Load the dataset\n",
                "file_path = 'D:\\\\Python_Projects\\\\attrition_predictor\\\\data\\\\HR_Dataset.csv'\n",
                "df = pd.read_csv(file_path)\n",
                "\n",
                "# Identify all duplicates, including their first occurrences\n",
                "all_duplicates = df.duplicated(keep=False)\n",
                "\n",
                "# Filter the DataFrame to show only duplicate rows\n",
                "duplicate_rows_df = df[all_duplicates]\n",
                "\n",
                "# Check if there are any duplicates\n",
                "if not duplicate_rows_df.empty:\n",
                "    # Sort the DataFrame by all columns to help in visually comparing the duplicate entries\n",
                "    sorted_duplicate_rows_df = duplicate_rows_df.sort_values(by=list(df.columns))\n",
                "\n",
                "    # Display the number of additional copies only\n",
                "    # We use df.duplicated() without keep=False to count only the additional copies\n",
                "    additional_copies_count = df.duplicated().sum()\n",
                "    print(f'Number of additional duplicate copies (excluding the first occurrence): {additional_copies_count}')\n",
                "    print('Duplicate Rows (including originals):')\n",
                "    print(sorted_duplicate_rows_df)\n",
                "\n",
                "    # Save to a CSV file if it doesn't exist\n",
                "    csv_file_path = 'duplicate_rows.csv'\n",
                "    if not os.path.exists(csv_file_path):\n",
                "        sorted_duplicate_rows_df.to_csv(csv_file_path, index=False)\n",
                "        print(f'All duplicate rows (including originals) have been saved to \"{csv_file_path}\".')\n",
                "    else:\n",
                "        print(f'The file \"{csv_file_path}\" already exists.')\n",
                "\n",
                "    # Remove all duplicate rows, keeping only the first occurrence\n",
                "    df = df.drop_duplicates()\n",
                "\n",
                "    # Confirm the removal\n",
                "    print(f'Number of rows after removing duplicates: {df.shape[0]}')\n",
                "else:\n",
                "    print('No duplicate rows found.')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Correlations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 53,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Strip trailing spaces from column names\n",
                "df.columns = df.columns.str.strip()\n",
                "\n",
                "# Correct column names\n",
                "department_col = 'Departments'\n",
                "salary_col = 'salary'\n",
                "\n",
                "# Encode categorical features into numerical values for correlation analysis\n",
                "df_encoded = df.copy()\n",
                "df_encoded[department_col] = df_encoded[department_col].astype('category').cat.codes\n",
                "df_encoded[salary_col] = df_encoded[salary_col].astype('category').cat.codes\n",
                "\n",
                "# Calculate the correlation matrix\n",
                "corr_matrix = df_encoded.corr(numeric_only=True)\n",
                "\n",
                "# Display the heatmap with annotations for all cells\n",
                "plt.figure(figsize=(12, 8))\n",
                "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', cbar=True, linewidths=0.5, linecolor='white')\n",
                "plt.title('Correlation Matrix with Annotations in All Cells')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Distribution and Relationships"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 54,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Distribution of target variable\n",
                "plt.figure(figsize=(6, 4))\n",
                "sns.countplot(x='left', data=df)\n",
                "plt.title('Distribution of Employees Leaving')\n",
                "plt.xlabel('Left Company')\n",
                "plt.ylabel('Count')\n",
                "plt.show()\n",
                "\n",
                "# Visualize relationships between features and target\n",
                "features = ['satisfaction_level', 'last_evaluation', 'number_project', 'average_montly_hours', 'time_spend_company']\n",
                "fig, axs = plt.subplots(2, 3, figsize=(15, 10))\n",
                "axs = axs.flatten()\n",
                "\n",
                "for i, feature in enumerate(features):\n",
                "    sns.boxplot(x='left', y=feature, data=df, ax=axs[i])\n",
                "    axs[i].set_title(f'{feature} vs Left')\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Visualize categorical features\n",
                "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
                "sns.countplot(x='salary', hue='left', data=df, ax=axs[0])\n",
                "sns.countplot(x='Departments', hue='left', data=df, ax=axs[1])\n",
                "axs[0].set_title('Salary vs Left')\n",
                "axs[1].set_title('Departments vs Left')\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Pair plot of selected features\n",
                "selected_features = ['satisfaction_level', 'last_evaluation', 'number_project', 'average_montly_hours', 'time_spend_company', 'left']\n",
                "sns.pairplot(df[selected_features], hue='left')\n",
                "plt.show()\n",
                "\n",
                "# Explore interesting insights\n",
                "# Employees with the lowest satisfaction levels (<= 0.3) and high monthly hours (> 250) are likely to leave\n",
                "high_hours_low_satisfaction = df[(df['satisfaction_level'] <= 0.3) & (df['average_montly_hours'] > 250)]\n",
                "plt.figure(figsize=(8, 6))\n",
                "sns.boxplot(x='left', y='average_montly_hours', data=high_hours_low_satisfaction)\n",
                "plt.title('Employees with Low Satisfaction and High Monthly Hours')\n",
                "plt.show()\n",
                "\n",
                "# Employees with multiple projects (>5) are likely to leave\n",
                "multiple_projects = df[(df['number_project'] > 5)]\n",
                "plt.figure(figsize=(8, 6))\n",
                "sns.countplot(x='left', data=multiple_projects)\n",
                "plt.title('Employees with Multiple Projects (> 5)')\n",
                "plt.show()\n",
                "\n",
                "# Plot the leave rate by average monthly hours\n",
                "def plot_left_rate_by_hours(df, bin_width=10):\n",
                "    bins = range(0, df['average_montly_hours'].max() + bin_width, bin_width)\n",
                "    df['hours_bin'] = pd.cut(df['average_montly_hours'], bins=bins)\n",
                "    grouped = df.groupby('hours_bin')['left'].mean()\n",
                "    count = df.groupby('hours_bin')['left'].count()\n",
                "\n",
                "    plt.figure(figsize=(12, 6))\n",
                "    grouped.plot(kind='bar', color='blue', alpha=0.6)\n",
                "    plt.xlabel('Average Monthly Hours')\n",
                "    plt.ylabel('Leave Rate')\n",
                "    plt.title('Leave Rate by Average Monthly Hours')\n",
                "    plt.xticks(rotation=45)\n",
                "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
                "    plt.show()\n",
                "\n",
                "# Plot leave rate by average monthly hours\n",
                "plot_left_rate_by_hours(df)\n",
                "\n",
                "# Plot leave rate by number of projects\n",
                "def plot_left_rate_by_projects(df):\n",
                "    grouped = df.groupby('number_project')['left'].mean()\n",
                "    count = df.groupby('number_project')['left'].count()\n",
                "\n",
                "    plt.figure(figsize=(10, 6))\n",
                "    grouped.plot(kind='bar', color='blue', alpha=0.6)\n",
                "    plt.xlabel('Number of Projects')\n",
                "    plt.ylabel('Leave Rate')\n",
                "    plt.title('Leave Rate by Number of Projects')\n",
                "    plt.xticks(rotation=0)\n",
                "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
                "    plt.show()\n",
                "\n",
                "# Plot leave rate by number of projects\n",
                "plot_left_rate_by_projects(df)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Prepare the Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 55,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define the feature columns and the target column for classification\n",
                "X_class = df.drop(columns=['left'])\n",
                "y_class = df['left']\n",
                "\n",
                "# Define the feature columns and the target column for regression\n",
                "X_reg = df.drop(columns=['satisfaction_level', 'left'])\n",
                "y_reg = df['satisfaction_level']\n",
                "\n",
                "# Identify categorical and numerical columns\n",
                "categorical_cols = X_class.select_dtypes(include=['object']).columns\n",
                "numerical_cols = X_class.select_dtypes(include=['int64', 'float64']).columns\n",
                "\n",
                "# Separate ordinal and nominal columns\n",
                "ordinal_cols = ['salary']\n",
                "nominal_cols = [col for col in categorical_cols if col not in ordinal_cols]\n",
                "\n",
                "# Define the ordinal encoder for the 'salary' column\n",
                "salary_categories = ['low', 'medium', 'high']\n",
                "ordinal_encoder = OrdinalEncoder(categories=[salary_categories])\n",
                "\n",
                "# Preprocessing pipeline for classification\n",
                "preprocessor_class = ColumnTransformer(\n",
                "    transformers=[\n",
                "        ('num', StandardScaler(), numerical_cols),\n",
                "        ('ord', ordinal_encoder, ordinal_cols),\n",
                "        ('nom', OneHotEncoder(drop='first', handle_unknown='ignore'), nominal_cols)\n",
                "    ]\n",
                ")\n",
                "\n",
                "# Preprocessing pipeline for regression\n",
                "preprocessor_reg = ColumnTransformer(\n",
                "    transformers=[\n",
                "        ('num', StandardScaler(), X_reg.select_dtypes(include=['int64', 'float64']).columns),\n",
                "        ('ord', ordinal_encoder, ordinal_cols),\n",
                "        ('nom', OneHotEncoder(drop='first', handle_unknown='ignore'), nominal_cols)\n",
                "    ]\n",
                ")\n",
                "\n",
                "# Split data into training and test sets for classification\n",
                "X_class_train, X_class_test, y_class_train, y_class_test = train_test_split(X_class, y_class, test_size=0.3, random_state=42)\n",
                "\n",
                "# Split data into training and test sets for regression\n",
                "X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(X_reg, y_reg, test_size=0.3, random_state=42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Resampling Techniques for Classification"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 56,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Applying Random OverSampler...\n",
                        "Resampled dataset shape: Counter({0: 7005, 1: 7005})\n",
                        "----------------------------------------\n",
                        "Applying SMOTE...\n",
                        "Resampled dataset shape: Counter({0: 7005, 1: 7005})\n",
                        "----------------------------------------\n",
                        "Applying Borderline SMOTE...\n",
                        "Resampled dataset shape: Counter({0: 7005, 1: 7005})\n",
                        "----------------------------------------\n",
                        "Applying ADASYN...\n",
                        "Resampled dataset shape: Counter({0: 7005, 1: 7003})\n",
                        "----------------------------------------\n"
                    ]
                }
            ],
            "source": [
                "# Function to resample data using different techniques\n",
                "def resample_data(X, y, sampler):\n",
                "    X_resampled, y_resampled = sampler.fit_resample(X, y)\n",
                "    print(f\"Resampled dataset shape: {Counter(y_resampled)}\")\n",
                "    return X_resampled, y_resampled\n",
                "\n",
                "# Apply resampling techniques\n",
                "samplers = {\n",
                "    'Random OverSampler': RandomOverSampler(random_state=42),\n",
                "    'SMOTE': SMOTE(random_state=42),\n",
                "    'Borderline SMOTE': BorderlineSMOTE(random_state=42),\n",
                "    'ADASYN': ADASYN(random_state=42)\n",
                "}\n",
                "\n",
                "X_class_train_transformed = preprocessor_class.fit_transform(X_class_train)\n",
                "\n",
                "resampled_datasets = {}\n",
                "for name, sampler in samplers.items():\n",
                "    print(f'Applying {name}...')\n",
                "    X_resampled, y_resampled = resample_data(X_class_train_transformed, y_class_train, sampler)\n",
                "    resampled_datasets[name] = (X_resampled, y_resampled)\n",
                "    print('-' * 40)\n",
                "\n",
                "# Also include the original dataset\n",
                "resampled_datasets['Original'] = (X_class_train_transformed, y_class_train)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Hyperparameter Tuning and Evaluation Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 57,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Hyperparameter grids\n",
                "param_grids_classification = {\n",
                "    'Logistic Regression': {'model__C': [0.01, 0.1, 1, 10]},\n",
                "    'Random Forest': {'model__n_estimators': [50, 100, 150], 'model__max_depth': [5, 10, 15]},\n",
                "    'XGBoost': {'model__n_estimators': [50, 100], 'model__max_depth': [3, 5, 7]},\n",
                "    'Gradient Boosting': {'model__n_estimators': [50, 100], 'model__max_depth': [3, 5]},\n",
                "    'K-Nearest Neighbors': {'model__n_neighbors': [3, 5, 7]}\n",
                "}\n",
                "\n",
                "param_grids_regression = {\n",
                "    'Linear Regression': {},\n",
                "    'Decision Tree': {'model__max_depth': [3, 5, 7]},\n",
                "    'Random Forest': {'model__n_estimators': [50, 100, 150], 'model__max_depth': [5, 10, 15]},\n",
                "    'XGBoost': {'model__n_estimators': [50, 100], 'model__max_depth': [3, 5, 7]},\n",
                "    'Gradient Boosting': {'model__n_estimators': [50, 100], 'model__max_depth': [3, 5]},\n",
                "    'K-Nearest Neighbors': {'model__n_neighbors': [3, 5, 7]}\n",
                "}\n",
                "\n",
                "# Function to find best hyperparameters for classification models\n",
                "def find_best_hyperparameters_classification(X_train, y_train, X_test, y_test):\n",
                "    models = {\n",
                "        'Logistic Regression': LogisticRegression(),\n",
                "        'Random Forest': RandomForestClassifier(random_state=42),\n",
                "        'XGBoost': XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
                "        'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
                "        'K-Nearest Neighbors': KNeighborsClassifier()\n",
                "    }\n",
                "\n",
                "    best_auc = 0\n",
                "    best_model_name = None\n",
                "    best_model = None\n",
                "    best_sampler = None\n",
                "    best_params = None\n",
                "    model_times = {}\n",
                "    for sampler_name, (X_resampled, y_resampled) in resampled_datasets.items():\n",
                "        print(f'### Evaluating models for {sampler_name} dataset ###')\n",
                "        for model_name, model in models.items():\n",
                "            start_time = time.time()\n",
                "            pipeline = Pipeline([\n",
                "                ('model', model)\n",
                "            ])\n",
                "            grid = GridSearchCV(pipeline, param_grids_classification[model_name], cv=3, scoring='roc_auc')\n",
                "            grid.fit(X_resampled, y_resampled)\n",
                "            end_time = time.time()\n",
                "            training_time = end_time - start_time\n",
                "            model_times[model_name] = training_time\n",
                "            y_pred_proba = grid.predict_proba(X_test)[:, 1]\n",
                "            auc = roc_auc_score(y_test, y_pred_proba)\n",
                "            print(f'{model_name}: ROC AUC Score = {auc:.3f}, Training Time: {training_time:.3f} seconds, Best Params: {grid.best_params_}')\n",
                "            if auc > best_auc:\n",
                "                best_auc = auc\n",
                "                best_model_name = model_name\n",
                "                best_model = grid.best_estimator_\n",
                "                best_sampler = sampler_name\n",
                "                best_params = grid.best_params_\n",
                "        print('-' * 50)\n",
                "    print(f'Best Classification Model: {best_model_name} with {best_sampler} (ROC AUC Score = {best_auc:.3f})')\n",
                "    print(f'Selected Hyperparameters: {best_params}')\n",
                "    print(f'Training Times: {model_times}')\n",
                "    return best_model_name, best_model, best_sampler, best_params, model_times\n",
                "\n",
                "# Function to find best hyperparameters for regression models\n",
                "def find_best_hyperparameters_regression(X_train, y_train, X_test, y_test):\n",
                "    models = {\n",
                "        'Linear Regression': LinearRegression(),\n",
                "        'Decision Tree': DecisionTreeRegressor(random_state=42),\n",
                "        'Random Forest': RandomForestRegressor(random_state=42),\n",
                "        'XGBoost': XGBRegressor(random_state=42),\n",
                "        'Gradient Boosting': GradientBoostingRegressor(random_state=42),\n",
                "        'K-Nearest Neighbors': KNeighborsRegressor()\n",
                "    }\n",
                "\n",
                "    best_r2 = -float('inf')\n",
                "    best_model_name = None\n",
                "    best_model = None\n",
                "    best_params = None\n",
                "    model_times = {}\n",
                "    for model_name, model in models.items():\n",
                "        start_time = time.time()\n",
                "        pipeline = Pipeline([\n",
                "            ('model', model)\n",
                "        ])\n",
                "        grid = GridSearchCV(pipeline, param_grids_regression[model_name], cv=3, scoring='r2')\n",
                "        grid.fit(X_train, y_train)\n",
                "        end_time = time.time()\n",
                "        training_time = end_time - start_time\n",
                "        model_times[model_name] = training_time\n",
                "        y_pred = grid.predict(X_test)\n",
                "        r2 = r2_score(y_test, y_pred)\n",
                "        print(f'{model_name}: R2 = {r2:.3f}, Training Time: {training_time:.3f} seconds, Best Params: {grid.best_params_}')\n",
                "        if r2 > best_r2:\n",
                "            best_r2 = r2\n",
                "            best_model_name = model_name\n",
                "            best_model = grid.best_estimator_\n",
                "            best_params = grid.best_params_\n",
                "    print(f'Best Regression Model: {best_model_name} (R2 Score = {best_r2:.3f})')\n",
                "    print(f'Selected Hyperparameters: {best_params}')\n",
                "    print(f'Training Times: {model_times}')\n",
                "    return best_model_name, best_model, best_params, model_times\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Cross-Validation Functions for Overfitting Check"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 58,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cross-validation function for classification\n",
                "def cross_val_classification(model, X, y):\n",
                "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
                "    auc_scores = cross_val_score(model, X, y, cv=skf, scoring='roc_auc')\n",
                "    print(f'Cross-Validation ROC AUC Scores: {auc_scores}')\n",
                "    print(f'Mean ROC AUC Score: {auc_scores.mean():.3f}')\n",
                "    print(f'Standard Deviation: {auc_scores.std():.3f}')\n",
                "    return auc_scores\n",
                "\n",
                "# Cross-validation function for regression\n",
                "def cross_val_regression(model, X, y):\n",
                "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
                "    r2_scores = cross_val_score(model, X, y, cv=kf, scoring='r2')\n",
                "    print(f'Cross-Validation R2 Scores: {r2_scores}')\n",
                "    print(f'Mean R2 Score: {r2_scores.mean():.3f}')\n",
                "    print(f'Standard Deviation: {r2_scores.std():.3f}')\n",
                "    return r2_scores\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Find Best Models and Perform Feature Importance Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 59,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "### Evaluating models for Random OverSampler dataset ###\n",
                        "Logistic Regression: ROC AUC Score = 0.845, Training Time: 0.271 seconds, Best Params: {'model__C': 0.1}\n",
                        "Random Forest: ROC AUC Score = 0.982, Training Time: 12.419 seconds, Best Params: {'model__max_depth': 15, 'model__n_estimators': 150}\n",
                        "XGBoost: ROC AUC Score = 0.980, Training Time: 1.477 seconds, Best Params: {'model__max_depth': 7, 'model__n_estimators': 100}\n",
                        "Gradient Boosting: ROC AUC Score = 0.981, Training Time: 9.686 seconds, Best Params: {'model__max_depth': 5, 'model__n_estimators': 100}\n",
                        "K-Nearest Neighbors: ROC AUC Score = 0.948, Training Time: 0.404 seconds, Best Params: {'model__n_neighbors': 7}\n",
                        "--------------------------------------------------\n",
                        "### Evaluating models for SMOTE dataset ###\n",
                        "Logistic Regression: ROC AUC Score = 0.844, Training Time: 0.284 seconds, Best Params: {'model__C': 0.1}\n",
                        "Random Forest: ROC AUC Score = 0.979, Training Time: 16.410 seconds, Best Params: {'model__max_depth': 15, 'model__n_estimators': 150}\n",
                        "XGBoost: ROC AUC Score = 0.980, Training Time: 1.735 seconds, Best Params: {'model__max_depth': 7, 'model__n_estimators': 100}\n",
                        "Gradient Boosting: ROC AUC Score = 0.983, Training Time: 14.242 seconds, Best Params: {'model__max_depth': 5, 'model__n_estimators': 100}\n",
                        "K-Nearest Neighbors: ROC AUC Score = 0.954, Training Time: 0.433 seconds, Best Params: {'model__n_neighbors': 7}\n",
                        "--------------------------------------------------\n",
                        "### Evaluating models for Borderline SMOTE dataset ###\n",
                        "Logistic Regression: ROC AUC Score = 0.787, Training Time: 0.327 seconds, Best Params: {'model__C': 0.01}\n",
                        "Random Forest: ROC AUC Score = 0.982, Training Time: 18.670 seconds, Best Params: {'model__max_depth': 15, 'model__n_estimators': 100}\n",
                        "XGBoost: ROC AUC Score = 0.980, Training Time: 1.785 seconds, Best Params: {'model__max_depth': 7, 'model__n_estimators': 50}\n",
                        "Gradient Boosting: ROC AUC Score = 0.981, Training Time: 17.052 seconds, Best Params: {'model__max_depth': 5, 'model__n_estimators': 100}\n",
                        "K-Nearest Neighbors: ROC AUC Score = 0.949, Training Time: 0.452 seconds, Best Params: {'model__n_neighbors': 7}\n",
                        "--------------------------------------------------\n",
                        "### Evaluating models for ADASYN dataset ###\n",
                        "Logistic Regression: ROC AUC Score = 0.815, Training Time: 0.296 seconds, Best Params: {'model__C': 0.01}\n",
                        "Random Forest: ROC AUC Score = 0.982, Training Time: 20.575 seconds, Best Params: {'model__max_depth': 15, 'model__n_estimators': 150}\n",
                        "XGBoost: ROC AUC Score = 0.979, Training Time: 1.962 seconds, Best Params: {'model__max_depth': 7, 'model__n_estimators': 100}\n",
                        "Gradient Boosting: ROC AUC Score = 0.984, Training Time: 17.331 seconds, Best Params: {'model__max_depth': 5, 'model__n_estimators': 100}\n",
                        "K-Nearest Neighbors: ROC AUC Score = 0.951, Training Time: 0.407 seconds, Best Params: {'model__n_neighbors': 7}\n",
                        "--------------------------------------------------\n",
                        "### Evaluating models for Original dataset ###\n",
                        "Logistic Regression: ROC AUC Score = 0.833, Training Time: 0.249 seconds, Best Params: {'model__C': 0.01}\n",
                        "Random Forest: ROC AUC Score = 0.980, Training Time: 8.174 seconds, Best Params: {'model__max_depth': 10, 'model__n_estimators': 100}\n",
                        "XGBoost: ROC AUC Score = 0.983, Training Time: 1.357 seconds, Best Params: {'model__max_depth': 3, 'model__n_estimators': 50}\n",
                        "Gradient Boosting: ROC AUC Score = 0.985, Training Time: 5.945 seconds, Best Params: {'model__max_depth': 3, 'model__n_estimators': 100}\n",
                        "K-Nearest Neighbors: ROC AUC Score = 0.953, Training Time: 0.202 seconds, Best Params: {'model__n_neighbors': 7}\n",
                        "--------------------------------------------------\n",
                        "Best Classification Model: Gradient Boosting with Original (ROC AUC Score = 0.985)\n",
                        "Selected Hyperparameters: {'model__max_depth': 3, 'model__n_estimators': 100}\n",
                        "Training Times: {'Logistic Regression': 0.24925017356872559, 'Random Forest': 8.17362117767334, 'XGBoost': 1.3569364547729492, 'Gradient Boosting': 5.944637298583984, 'K-Nearest Neighbors': 0.20232057571411133}\n",
                        "\n",
                        "Cross-Validation Results (Classification Model)\n",
                        "Cross-Validation ROC AUC Scores: [0.98399653 0.98642157 0.98434315 0.97908791 0.99083558]\n",
                        "Mean ROC AUC Score: 0.985\n",
                        "Standard Deviation: 0.004\n",
                        "Accuracy: 0.980\n",
                        "Precision: 0.960\n",
                        "Recall: 0.920\n",
                        "F1 Score: 0.940\n",
                        "ROC AUC Score: 0.985\n"
                    ]
                }
            ],
            "source": [
                "# Find best classification model\n",
                "best_class_model_name, best_class_model, best_class_sampler, best_class_params, class_model_times = find_best_hyperparameters_classification(\n",
                "    preprocessor_class.transform(X_class_train),\n",
                "    y_class_train,\n",
                "    preprocessor_class.transform(X_class_test),\n",
                "    y_class_test\n",
                ")\n",
                "\n",
                "# Extract feature names from the preprocessing pipeline for classification\n",
                "feature_names_num = numerical_cols.tolist()\n",
                "feature_names_ord = ordinal_cols\n",
                "feature_names_nom = preprocessor_class.named_transformers_['nom'].get_feature_names_out(nominal_cols)\n",
                "\n",
                "all_feature_names = np.concatenate([feature_names_num, feature_names_ord, feature_names_nom])\n",
                "\n",
                "# Ensure resampled data remains a DataFrame\n",
                "X_resampled, y_resampled = resampled_datasets[best_class_sampler]\n",
                "X_resampled_df = pd.DataFrame(X_resampled, columns=all_feature_names)\n",
                "\n",
                "# Fit the best model\n",
                "best_class_model.fit(X_resampled_df, y_resampled)\n",
                "\n",
                "# Perform cross-validation for classification model\n",
                "print('\\nCross-Validation Results (Classification Model)')\n",
                "cross_val_classification(best_class_model, X_resampled_df, y_resampled)\n",
                "\n",
                "# Extract feature importance\n",
                "feature_importances = best_class_model.named_steps['model'].feature_importances_\n",
                "\n",
                "# Create a DataFrame for visualization\n",
                "importance_df = pd.DataFrame({'Feature': all_feature_names, 'Importance': feature_importances})\n",
                "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
                "\n",
                "# Plot the feature importance\n",
                "plt.figure(figsize=(10, 8))\n",
                "sns.barplot(x='Importance', y='Feature', data=importance_df)\n",
                "plt.title('Feature Importance Analysis (Classification)')\n",
                "plt.show()\n",
                "\n",
                "importance_df.head(10)\n",
                "\n",
                "# Confusion matrix\n",
                "y_pred_class = best_class_model.predict(preprocessor_class.transform(X_class_test))\n",
                "cm = confusion_matrix(y_class_test, y_pred_class)\n",
                "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_class_model.classes_)\n",
                "disp.plot(cmap='Blues', xticks_rotation='vertical')\n",
                "plt.title('Confusion Matrix (Classification)')\n",
                "plt.show()\n",
                "\n",
                "# Evaluation metrics\n",
                "accuracy = accuracy_score(y_class_test, y_pred_class)\n",
                "precision = precision_score(y_class_test, y_pred_class)\n",
                "recall = recall_score(y_class_test, y_pred_class)\n",
                "f1 = f1_score(y_class_test, y_pred_class)\n",
                "roc_auc = roc_auc_score(y_class_test, best_class_model.predict_proba(preprocessor_class.transform(X_class_test))[:, 1])\n",
                "\n",
                "print(f'Accuracy: {accuracy:.3f}')\n",
                "print(f'Precision: {precision:.3f}')\n",
                "print(f'Recall: {recall:.3f}')\n",
                "print(f'F1 Score: {f1:.3f}')\n",
                "print(f'ROC AUC Score: {roc_auc:.3f}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 60,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Linear Regression: R2 = 0.049, Training Time: 0.027 seconds, Best Params: {}\n",
                        "Decision Tree: R2 = 0.347, Training Time: 0.076 seconds, Best Params: {'model__max_depth': 5}\n",
                        "Random Forest: R2 = 0.381, Training Time: 22.166 seconds, Best Params: {'model__max_depth': 10, 'model__n_estimators': 150}\n",
                        "XGBoost: R2 = 0.361, Training Time: 1.416 seconds, Best Params: {'model__max_depth': 3, 'model__n_estimators': 50}\n",
                        "Gradient Boosting: R2 = 0.370, Training Time: 4.362 seconds, Best Params: {'model__max_depth': 5, 'model__n_estimators': 50}\n",
                        "K-Nearest Neighbors: R2 = 0.274, Training Time: 0.167 seconds, Best Params: {'model__n_neighbors': 7}\n",
                        "Best Regression Model: Random Forest (R2 Score = 0.381)\n",
                        "Selected Hyperparameters: {'model__max_depth': 10, 'model__n_estimators': 150}\n",
                        "Training Times: {'Linear Regression': 0.02700352668762207, 'Decision Tree': 0.07552909851074219, 'Random Forest': 22.165920972824097, 'XGBoost': 1.4159417152404785, 'Gradient Boosting': 4.361708164215088, 'K-Nearest Neighbors': 0.1665339469909668}\n",
                        "\n",
                        "Cross-Validation Results (Regression Model)\n",
                        "Cross-Validation R2 Scores: [0.3623367  0.33821095 0.35497783 0.36606101 0.41332155]\n",
                        "Mean R2 Score: 0.367\n",
                        "Standard Deviation: 0.025\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>Feature</th>\n",
                            "      <th>Importance</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>number_project</td>\n",
                            "      <td>0.468619</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>average_montly_hours</td>\n",
                            "      <td>0.209398</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>last_evaluation</td>\n",
                            "      <td>0.147792</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>time_spend_company</td>\n",
                            "      <td>0.095638</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>6</th>\n",
                            "      <td>salary</td>\n",
                            "      <td>0.016914</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>Work_accident</td>\n",
                            "      <td>0.009023</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>15</th>\n",
                            "      <td>Departments_technical</td>\n",
                            "      <td>0.008205</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>13</th>\n",
                            "      <td>Departments_sales</td>\n",
                            "      <td>0.007509</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>14</th>\n",
                            "      <td>Departments_support</td>\n",
                            "      <td>0.007437</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>11</th>\n",
                            "      <td>Departments_marketing</td>\n",
                            "      <td>0.005798</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "                  Feature  Importance\n",
                            "1          number_project    0.468619\n",
                            "2    average_montly_hours    0.209398\n",
                            "0         last_evaluation    0.147792\n",
                            "3      time_spend_company    0.095638\n",
                            "6                  salary    0.016914\n",
                            "4           Work_accident    0.009023\n",
                            "15  Departments_technical    0.008205\n",
                            "13      Departments_sales    0.007509\n",
                            "14    Departments_support    0.007437\n",
                            "11  Departments_marketing    0.005798"
                        ]
                    },
                    "execution_count": 60,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Find best regression model\n",
                "best_reg_model_name, best_reg_model, best_reg_params, reg_model_times = find_best_hyperparameters_regression(\n",
                "    preprocessor_reg.fit_transform(X_reg_train),\n",
                "    y_reg_train,\n",
                "    preprocessor_reg.transform(X_reg_test),\n",
                "    y_reg_test\n",
                ")\n",
                "\n",
                "# Extract feature names from the preprocessing pipeline for regression\n",
                "feature_names_num_reg = X_reg.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
                "feature_names_ord = ordinal_cols\n",
                "feature_names_nom = preprocessor_reg.named_transformers_['nom'].get_feature_names_out(nominal_cols)\n",
                "\n",
                "all_feature_names_reg = np.concatenate([feature_names_num_reg, feature_names_ord, feature_names_nom])\n",
                "\n",
                "# Fit the best model\n",
                "best_reg_model.fit(preprocessor_reg.fit_transform(X_reg_train), y_reg_train)\n",
                "\n",
                "# Perform cross-validation for regression model\n",
                "print('\\nCross-Validation Results (Regression Model)')\n",
                "cross_val_regression(best_reg_model, preprocessor_reg.transform(X_reg_train), y_reg_train)\n",
                "\n",
                "# Extract feature importance\n",
                "feature_importances_reg = best_reg_model.named_steps['model'].feature_importances_\n",
                "\n",
                "# Create a DataFrame for visualization\n",
                "importance_df_reg = pd.DataFrame({'Feature': all_feature_names_reg, 'Importance': feature_importances_reg})\n",
                "importance_df_reg = importance_df_reg.sort_values(by='Importance', ascending=False)\n",
                "\n",
                "# Plot the feature importance\n",
                "plt.figure(figsize=(10, 8))\n",
                "sns.barplot(x='Importance', y='Feature', data=importance_df_reg)\n",
                "plt.title('Feature Importance Analysis (Regression)')\n",
                "plt.show()\n",
                "\n",
                "importance_df_reg.head(10)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Check Multicollinearity with VIF (Variance Inflation Factor)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 61,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Variance Inflation Factor (VIF) - Classification Model:\n",
                        "                    Feature       VIF\n",
                        "1        satisfaction_level  1.068027\n",
                        "2           last_evaluation  1.146947\n",
                        "3            number_project  1.221450\n",
                        "4      average_montly_hours  1.160250\n",
                        "5        time_spend_company  1.084388\n",
                        "6             Work_accident  1.003996\n",
                        "7     promotion_last_5years  1.029254\n",
                        "8                    salary  1.025393\n",
                        "9         Departments_RandD  1.615933\n",
                        "10   Departments_accounting  1.548496\n",
                        "11           Departments_hr  1.534355\n",
                        "12   Departments_management  1.471403\n",
                        "13    Departments_marketing  1.624864\n",
                        "14  Departments_product_mng  1.608088\n",
                        "15        Departments_sales  3.147648\n",
                        "16      Departments_support  2.400642\n",
                        "17    Departments_technical  2.664458\n",
                        "\n",
                        "Variance Inflation Factor (VIF) - Regression Model:\n",
                        "                    Feature       VIF\n",
                        "1           last_evaluation  1.122778\n",
                        "2            number_project  1.194973\n",
                        "3      average_montly_hours  1.159926\n",
                        "4        time_spend_company  1.060021\n",
                        "5             Work_accident  1.003207\n",
                        "6     promotion_last_5years  1.028597\n",
                        "7                    salary  1.024163\n",
                        "8         Departments_RandD  1.615933\n",
                        "9    Departments_accounting  1.548108\n",
                        "10           Departments_hr  1.533828\n",
                        "11   Departments_management  1.471376\n",
                        "12    Departments_marketing  1.624864\n",
                        "13  Departments_product_mng  1.608081\n",
                        "14        Departments_sales  3.147635\n",
                        "15      Departments_support  2.400616\n",
                        "16    Departments_technical  2.664397\n"
                    ]
                }
            ],
            "source": [
                "# Function to calculate VIF\n",
                "def calculate_vif(X):\n",
                "    X = sm.add_constant(X)\n",
                "    vif_df = pd.DataFrame()\n",
                "    vif_df['Feature'] = X.columns\n",
                "    vif_df['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
                "    return vif_df[vif_df['Feature'] != 'const']\n",
                "\n",
                "# VIF for classification model\n",
                "X_class_vif = preprocessor_class.fit_transform(X_class_train)\n",
                "X_class_vif_df = pd.DataFrame(X_class_vif, columns=all_feature_names)\n",
                "vif_class = calculate_vif(X_class_vif_df)\n",
                "\n",
                "# VIF for regression model\n",
                "X_reg_vif = preprocessor_reg.fit_transform(X_reg_train)\n",
                "X_reg_vif_df = pd.DataFrame(X_reg_vif, columns=all_feature_names_reg)\n",
                "vif_reg = calculate_vif(X_reg_vif_df)\n",
                "\n",
                "# Display VIF results\n",
                "print('Variance Inflation Factor (VIF) - Classification Model:')\n",
                "print(vif_class)\n",
                "print('\\nVariance Inflation Factor (VIF) - Regression Model:')\n",
                "print(vif_reg)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
